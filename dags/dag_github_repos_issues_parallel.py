from datetime import timedelta
from airflow.decorators import dag

import dlt
from dlt.common import pendulum
from dlt.helpers.airflow_helper import PipelineTasksGroup


# modify the default task arguments - all the tasks created for dlt pipeline will inherit it
# - set e-mail notifications
# - we set retries to 0 and recommend to use `PipelineTasksGroup` retry policies with tenacity library, you can also retry just extract and load steps
# - execution_timeout is set to 20 hours, tasks running longer that that will be terminated

default_task_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': 'test@test.com',
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'execution_timeout': timedelta(hours=20),
}

# modify the default DAG arguments
# - the schedule below sets the pipeline to `@daily` be run each day after midnight, you can use crontab expression instead
# - start_date - a date from which to generate backfill runs
# - catchup is False which means that the daily runs from `start_date` will not be run, set to True to enable backfill
# - max_active_runs - how many dag runs to perform in parallel. you should always start with 1


@dag(
    dag_id="github_dag_parallel",
    schedule_interval='@daily',
    start_date=pendulum.datetime(2023, 7, 1),
    catchup=False,
    max_active_runs=1,
    default_args=default_task_args
)
def load_data():
    from airflow.models import Variable
    import os

    # set `use_data_folder` to True to store temporary data on the `data` bucket. Use only when it does not fit on the local storage
    tasks = PipelineTasksGroup("pipeline_decomposed", use_data_folder=False, wipe_local_data=True)

    # import your source from pipeline script
    import sys
    sys.path.insert(0, '/app/src')
    from airflow_dlt_pipeline import github_source

    # Set AWS credentials as environment variables for dlt
    os.environ['AWS_ACCESS_KEY_ID'] = Variable.get("aws_access_key_id")
    os.environ['AWS_SECRET_ACCESS_KEY'] = Variable.get("aws_secret_access_key")
    os.environ['AWS_SESSION_TOKEN'] = Variable.get("aws_session_token")
    os.environ['AWS_DEFAULT_REGION'] = Variable.get("aws_region")

    # Set bucket URL as environment variable for dlt
    os.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = Variable.get("s3_bucket_url")

    # modify the pipeline parameters
    pipeline = dlt.pipeline(
        pipeline_name='github_repos_issues',
        dataset_name='github_data_parallel',
        destination='filesystem',
        full_refresh=False, # must be false if we decompose
    )

    # create the source, the "serialize" decompose option will converts dlt resources into Airflow tasks. use "none" to disable it
    tasks.add_run(pipeline, github_source, decompose="parallel-isolated", trigger_rule="all_done", retries=0, provide_context=True)


load_data()